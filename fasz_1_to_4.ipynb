{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community chromadb sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01014066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# 1. Load Documents (same as tutorial, you can replace the URL with local files)\n",
    "docs = [\n",
    "    {\"content\": \"My first dogs name was Shadow.\"},\n",
    "    {\"content\": \"Shadow was a black dog with a white patch on his chest.\"}\n",
    "]\n",
    "\n",
    "# 2. Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_text(\" \".join(doc['content'] for doc in docs))\n",
    "print(\"Splits:\", splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a95a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate Local Embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Fast, lightweight model all-mpnet-base-v2 larger more accurate model\n",
    "embeddings = [embedding_model.encode(split) for split in splits]\n",
    "print(\"Embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb6d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Store Embeddings Locally (using ChromaDB)\n",
    "import chromadb\n",
    "#\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Initialize Chroma client (persistent or in-memory)\n",
    "# client = chromadb.Client(Settings(persist_directory=\"chroma_storage\", chroma_db_impl=\"duckdb+parquet\"))\n",
    "\n",
    "# Create a collection in Chroma\n",
    "collection = client.get_or_create_collection(\"rag_local_demo\")\n",
    "\n",
    "# Add documents and embeddings\n",
    "for i, split in enumerate(splits):\n",
    "    collection.add(\n",
    "        ids=[str(i)],\n",
    "        documents=[split],\n",
    "        embeddings=[embeddings[i]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de913103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RETRIEVAL ####\n",
    "\n",
    "# Query Embedding\n",
    "query = \"What was the name of my first dog and what was their appearance?\"\n",
    "query_embedding = embedding_model.encode(query)\n",
    "\n",
    "# Retrieve relevant documents\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5  # Retrieve top 5 matches\n",
    ")\n",
    "print(\"Retrieved Documents:\", results[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2aceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GENERATION ####\n",
    "\n",
    "# 5. Use llama.cpp for Local LLM Inference\n",
    "def query_llama_cpp(input_text):\n",
    "    llama_path = \"/home/fasz/tools/llama.cpp/build/bin/llama-cli\"  # Path to llama.cpp binary\n",
    "    #model_path = \"/home/fasz/tools/models/TheBloke/Wizard-Vicuna-13B-Uncensored-GGUF/Wizard-Vicuna-13B-Uncensored.Q4_K_S.gguf\"\n",
    "    model_path = \"/home/fasz/tools/models/TheBloke/Wizard-Vicuna-30B-Uncensored-GGUF/Wizard-Vicuna-30B-Uncensored.Q4_K_M.gguf\"\n",
    "\n",
    "    command = [\n",
    "        llama_path,\n",
    "        \"--model\", model_path,\n",
    "        \"--prompt\", input_text,\n",
    "        \"--temp\", \"0.7\",        # Adjust temperature\n",
    "        \"--predict\", \"512\",     # Number of tokens to predict\n",
    "        \"--threads\", \"14\",      # Number of CPU threads\n",
    "        \"--gpu-layers\", \"10\",   # Number of layers to store in VRAM\n",
    "        \"--verbose\",\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    return result.stdout.strip()\n",
    "\n",
    "\n",
    "# Generate Answer\n",
    "retrieved_docs = \"\\n\".join([doc[0] for doc in results[\"documents\"]])\n",
    "query_text = f\"Context:\\n{retrieved_docs}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "response = query_llama_cpp(query_text)\n",
    "print(\"Generated Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1a47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = \"\\n\".join([doc[0] for doc in results[\"documents\"]])\n",
    "query_text = prompt.format(context=retrieved_docs, question=query)\n",
    "response = query_llama_cpp(query_text)\n",
    "print(\"Generated Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9891f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da1dc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "question = \"What was the name of my first dog and what was their appearance?\"\n",
    "num_of_tokens = num_tokens_from_string(question, \"cl100k_base\")\n",
    "print(f\"Number of tokens in question: {num_of_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc75b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "document = \"My first birds name was Tweety.\"\n",
    "question = \"What was the name of my first bird?\"\n",
    "document_embedding = embedding_model.encode(document)\n",
    "question_embedding = embedding_model.encode(question)\n",
    "similarity = cosine_similarity(question_embedding, document_embedding)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78d862",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f119c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "42c8b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(name=\"demo_documents\")\n",
    "documents = [split.page_content for split in splits]\n",
    "metadatas = [split.metadata for split in splits]  # Optional metadata\n",
    "embeddings = [embedding_model.encode(doc) for doc in documents]\n",
    "ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=documents,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadatas\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the collection\n",
    "query = \"What is an autonomous agent?\"\n",
    "#query = \"What is faszrohdan?\"\n",
    "query_embedding = embedding_model.encode(query)\n",
    "\n",
    "# Retrieve top 5 matching documents\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=5\n",
    ")\n",
    "documents = results[\"documents\"]\n",
    "# Print retrieved documents\n",
    "print(\"Retrieved Documents:\")\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "# Prompt\n",
    "#template = \"\"\"Try to answer the question based only on the following context or if the answer is not in context say \"I don't know\":\n",
    "#{context}\n",
    "#\n",
    "#Qestion: {question}\n",
    "#\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c0b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = \"\\n\".join([doc[0] for doc in documents])\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "565d2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = prompt.format(context=retrieved_docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_llama_cpp(query_text)\n",
    "print(\"Generated Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
